{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    train_data = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename), cv2.IMREAD_GRAYSCALE) # Convert to Image to Grayscale\n",
    "        img = ~img # Invert the bits of image 255 -> 0\n",
    "        if img is not None:\n",
    "            _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) # Set bits > 127 to 1 and <= 127 to 0\n",
    "            ctrs, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            cnt = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0]) # Sort by x\n",
    "            w = int(28)\n",
    "            h = int(28)\n",
    "            maxi = 0\n",
    "            for c in cnt:\n",
    "                x, y, w, h = cv2.boundingRect(c)\n",
    "                maxi = max(w*h, maxi)\n",
    "                if maxi == w*h:\n",
    "                    x_max = x\n",
    "                    y_max = y\n",
    "                    w_max = w\n",
    "                    h_max = h\n",
    "            im_crop = thresh[y_max:y_max+h_max+10, x_max:x_max+w_max+10] # Crop the image as most as possible\n",
    "            im_resize = cv2.resize(im_crop, (28, 28)) # Resize to (28, 28)\n",
    "            im_resize = np.reshape(im_resize, (784, 1)) # Flat the matrix\n",
    "            train_data.append(im_resize)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign '-'=10\n",
    "data=load_images_from_folder('./extracted_images/-')\n",
    "for i in range(0,len(data)):\n",
    "    data[i]=np.append(data[i],['10'])\n",
    "    \n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign '+'=11\n",
    "data11=load_images_from_folder('./extracted_images/+')\n",
    "for i in range(0,len(data11)):\n",
    "    data11[i]=np.append(data11[i],['11'])\n",
    "data=np.concatenate((data,data11))    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign '/'=13\n",
    "data13=load_images_from_folder('./extracted_images/div')\n",
    "for i in range(0,len(data13)):\n",
    "    data13[i]=np.append(data13[i],['13'])\n",
    "data=np.concatenate((data,data13))    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=load_images_from_folder('./extracted_images/0')\n",
    "for i in range(0,len(data0)):\n",
    "    data0[i]=np.append(data0[i],['0'])\n",
    "data=np.concatenate((data,data0))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=load_images_from_folder('./extracted_images/1')\n",
    "\n",
    "for i in range(0,len(data1)):\n",
    "    data1[i]=np.append(data1[i],['1'])\n",
    "data=np.concatenate((data,data1))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=load_images_from_folder('./extracted_images/2')\n",
    "\n",
    "for i in range(0,len(data2)):\n",
    "    data2[i]=np.append(data2[i],['2'])\n",
    "data=np.concatenate((data,data2))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=load_images_from_folder('./extracted_images/3')\n",
    "\n",
    "for i in range(0,len(data3)):\n",
    "    data3[i]=np.append(data3[i],['3'])\n",
    "data=np.concatenate((data,data3))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=load_images_from_folder('./extracted_images/4')\n",
    "\n",
    "for i in range(0,len(data4)):\n",
    "    data4[i]=np.append(data4[i],['4'])\n",
    "data=np.concatenate((data,data4))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5=load_images_from_folder('./extracted_images/5')\n",
    "\n",
    "for i in range(0,len(data5)):\n",
    "    data5[i]=np.append(data5[i],['5'])\n",
    "data=np.concatenate((data,data5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6=load_images_from_folder('./extracted_images/6')\n",
    "\n",
    "for i in range(0,len(data6)):\n",
    "    data6[i]=np.append(data6[i],['6'])\n",
    "data=np.concatenate((data,data6))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7=load_images_from_folder('./extracted_images/7')\n",
    "\n",
    "for i in range(0,len(data7)):\n",
    "    data7[i]=np.append(data7[i],['7'])\n",
    "data=np.concatenate((data,data7))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8=load_images_from_folder('./extracted_images/8')\n",
    "\n",
    "for i in range(0,len(data8)):\n",
    "    data8[i]=np.append(data8[i],['8'])\n",
    "data=np.concatenate((data,data8))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data9=load_images_from_folder('./extracted_images/9')\n",
    "\n",
    "for i in range(0,len(data9)):\n",
    "    data9[i]=np.append(data9[i],['9'])\n",
    "data=np.concatenate((data,data9))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign * = 12\n",
    "data12=load_images_from_folder('./extracted_images/times')\n",
    "\n",
    "for i in range(0,len(data12)):\n",
    "    data12[i]=np.append(data12[i],['12'])\n",
    "data=np.concatenate((data,data12))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,index=None)\n",
    "print(df.head)\n",
    "df.to_csv('train_div.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from os.path import isfile, join\n",
    "from keras import backend as K\n",
    "from os import listdir\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train_div.csv',index_col=False)\n",
    "labels=df_train[['784']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(df_train.columns[[784]],axis=1,inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels=np.array(labels)\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train=to_categorical(labels,num_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for i in range(len(df_train)):\n",
    "    x_train.append(np.array(df_train[i:i+1]).reshape(1, 28, 28))\n",
    "x_train = np.array(x_train)\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "x_train, X_val, y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.35, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(30,5,input_shape=(28,28,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(15,3,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(14, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "          x_train,\n",
    "          y_train,\n",
    "          epochs=5,\n",
    "          batch_size=64,\n",
    "          shuffle=True,\n",
    "          validation_data = (X_val, Y_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "#legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(14)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model-final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model-final.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Read\n",
      "Loaded\n",
      "Closed\n",
      "---\n",
      "Loading weights...\n",
      "Loaded weights\n"
     ]
    }
   ],
   "source": [
    "print('Loading Model...')\n",
    "model_json = open('./model/mod2.json', 'r')\n",
    "\n",
    "print(\"Read\")\n",
    "loaded_model_json = model_json.read()\n",
    "\n",
    "print(\"Loaded\")\n",
    "model_json.close()\n",
    "\n",
    "print(\"Closed\")\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print('Loading weights...')\n",
    "loaded_model.load_weights(\"./model/mod2.h5\")\n",
    "\n",
    "print(\"Loaded weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('./test3.png',cv2.IMREAD_GRAYSCALE)\n",
    "#cv2.imshow('wo',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image reading\n",
      "Image read\n"
     ]
    }
   ],
   "source": [
    "if img is not None:\n",
    "    print(\"Image reading\")\n",
    "    img = ~img # Invert the bits of image 255 -> 0\n",
    "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) # Set bits > 127 to 1 and <= 127 to 0\n",
    "    ctrs, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0]) # Sort by x\n",
    "    w=int(28)\n",
    "    h=int(28)\n",
    "\n",
    "    train_data = []\n",
    "    rects = []\n",
    "    for c in cnt :\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        rect = [x, y, w, h]\n",
    "        rects.append(rect)\n",
    "\n",
    "    bool_rect = []\n",
    "    # Check when two rectangles collide\n",
    "    for r in rects:\n",
    "        l = []\n",
    "        for rec in rects:\n",
    "            flag = 0\n",
    "            if rec != r:\n",
    "                if r[0] < (rec[0] + rec[2] + 10) and rec[0] < (r[0] + r[2] + 10) and r[1] < (rec[1] + rec[3] + 10) and rec[1] < (r[1] + r[3] + 10):\n",
    "                    flag = 1\n",
    "                l.append(flag)\n",
    "            else:\n",
    "                l.append(0)\n",
    "        bool_rect.append(l)\n",
    "\n",
    "    dump_rect = []\n",
    "    # Discard the small collide rectangle\n",
    "    for i in range(0, len(cnt)):\n",
    "        for j in range(0, len(cnt)):\n",
    "            if bool_rect[i][j] == 1:\n",
    "                area1 = rects[i][2] * rects[i][3]\n",
    "                area2 = rects[j][2] * rects[j][3]\n",
    "                if(area1 == min(area1,area2)):\n",
    "                    dump_rect.append(rects[i])\n",
    "\n",
    "    # Get the final rectangles\n",
    "    final_rect = [i for i in rects if i not in dump_rect]\n",
    "    for r in final_rect:\n",
    "        x = r[0]\n",
    "        y = r[1]\n",
    "        w = r[2]\n",
    "        h = r[3]\n",
    "\n",
    "        im_crop = thresh[y:y+h+10, x:x+w+10] # Crop the image as most as possible\n",
    "        im_resize = cv2.resize(im_crop, (28, 28)) # Resize to (28, 28)\n",
    "        im_resize = np.reshape(im_resize, (-1,28, 28,1)) # Flat the matrix\n",
    "        train_data.append(im_resize)\n",
    "    print(\"Image read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jitesh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45*67\n"
     ]
    }
   ],
   "source": [
    "s=''\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i]=np.array(train_data[i])\n",
    "    train_data[i]=train_data[i].reshape(-1,28,28,1)\n",
    "    result=loaded_model.predict_classes(train_data[i])\n",
    "    if(result[0]==10):\n",
    "        s=s+'-'\n",
    "    if(result[0]==11):\n",
    "        s=s+'+'\n",
    "    if(result[0]==12):\n",
    "        s=s+'*'\n",
    "    if(result[0]==0):\n",
    "        s=s+'0'\n",
    "    if(result[0]==1):\n",
    "        s=s+'1'\n",
    "    if(result[0]==2):\n",
    "        s=s+'2'\n",
    "    if(result[0]==3):\n",
    "        s=s+'3'\n",
    "    if(result[0]==4):\n",
    "        s=s+'4'\n",
    "    if(result[0]==5):\n",
    "        s=s+'5'\n",
    "    if(result[0]==6):\n",
    "        s=s+'6'\n",
    "    if(result[0]==7):\n",
    "        s=s+'7'\n",
    "    if(result[0]==8):\n",
    "        s=s+'8'\n",
    "    if(result[0]==9):\n",
    "        s=s+'9'\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3015"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
